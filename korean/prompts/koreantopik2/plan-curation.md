# TOPIK 2 Vocabulary Curation Plan

**Goal**: Filter 3,873 TOPIK 2 words down to ~1,000 high-value vocabulary items.

**Status**: üöß Planning phase

## Problem Statement

The raw koreantopik.com TOPIK 2 list (3,873 words) contains:

1. **Trivial English loanwords** - Zero learning value for English speakers
   - Examples: ÌÜ†ÎßàÌÜ†, ÌÖåÎãàÏä§, ÏïÑÏù¥Ïä§ÌÅ¨Î¶º, ÏΩúÎùº, ÏºÄÏù¥ÌÅ¨
   - Already known; just Hangul spelling recognition

2. **Redundant compositional compounds** - Predictable from roots
   - Examples: Í∞êÍ∏∞ ‚Üí Í∞êÍ∏∞ÏïΩ, Í¥ÄÍ¥ë ‚Üí Í¥ÄÍ¥ëÍ∞ù ‚Üí Í¥ÄÍ¥ëÏßÄ
   - If you know roots + suffixes, compounds are derivable
   - Example sentences naturally teach compounds anyway

## Filtering Strategy

### Pass 1: Trivial Loanword Removal (Automatic)

**Criteria**: Remove words where English ‚Üí Korean is phonetically obvious.

**Approach**:
- Romanize Korean word
- Compare to common English words
- Flag if similarity > threshold

**Exceptions to keep**:
- Loanwords with shifted meaning (ÏïÑÎ•¥Î∞îÏù¥Ìä∏ = part-time job, not "work")
- Loanwords with Korean-specific usage patterns
- Loanwords where Korean pronunciation differs significantly

**Implementation**: `scripts/filter-loanwords.py`

### Pass 2: Compositional Compound Removal (Semi-automatic)

**Criteria**: Keep root words, remove predictable derivatives.

**Approach**:
- Morphological analysis (Korean NLP tools)
- Identify N-morpheme compounds
- Check if meaning = sum of parts
- Keep root, skip derivatives

**Exceptions to keep**:
- High-frequency compounds that function as semantic units
- Compounds with non-compositional meaning (ÏÉùÏùº ‚â† ÏÉù + Ïùº)
- Idiomatic compounds

**Implementation**: `scripts/filter-compounds.py`

### Pass 3: Duplicate Removal (Automatic)

**Criteria**: Remove words already in existing Anki decks (TOPIK 1, etc.)

**Approach**:
- Export existing deck's `korean` field (see `anki/prompts/check-duplicates.md`)
- Filter out exact matches from TOPIK 2 list
- Handle homonyms carefully (same korean, different english = keep)

**Note**: 105 words overlap between TOPIK 1 and TOPIK 2 per existing documentation.

**Implementation**: `scripts/filter-duplicates.py`

### Pass 4: Frequency Validation (Automatic)

**Criteria**: Prioritize words by real-world usage frequency.

**Data sources**:
- Íµ≠Î¶ΩÍµ≠Ïñ¥Ïõê (National Institute of Korean Language) Sejong Corpus
- Korean subtitle frequency lists
- "A Frequency Dictionary of Korean" (Routledge)

**Approach**:
- Cross-reference filtered words with frequency rankings
- Sort by actual usage
- Keep top 1,000-1,500

**Implementation**: `scripts/filter-by-frequency.py`

## Pipeline

```
input/koreantopik2.tsv (3,873 words)
    ‚îÇ
    ‚ñº
[Pass 1: Loanword filter]
    ‚îÇ
    ‚ñº
[Pass 2: Compound filter]
    ‚îÇ
    ‚ñº
[Pass 3: Duplicate filter (vs existing Anki decks)]
    ‚îÇ
    ‚ñº
[Pass 4: Frequency ranking]
    ‚îÇ
    ‚ñº
output/koreantopik2-curated.tsv (~1,000 words)
```

## Output

**Target**: ~1,000 high-value words with:
- Core Korean vocabulary (not transparent loanwords)
- Root words or non-compositional compounds
- High frequency in actual usage
- Genuine learning value (new concept/pattern/collocation)

## Next Steps

1. [ ] Export existing Anki deck korean fields for duplicate detection
2. [ ] Find/download Korean frequency data (Íµ≠Î¶ΩÍµ≠Ïñ¥Ïõê or alternative)
3. [ ] Prototype loanword detection heuristic
4. [ ] Test on sample of TOPIK 2 words
5. [ ] Iterate filters based on results
6. [ ] Generate curated list
7. [ ] Run through existing enhancement pipeline (etymology, examples, notes, audio)

## Open Questions

- What frequency threshold to use?
- How to handle borderline loanwords (partially shifted meaning)?
- How to handle homonyms in duplicate filtering (same korean, different meaning)?
- Manual review step for edge cases?

## Resources

- koreantopik.com TOPIK 2 source: https://www.koreantopik.com/2024/09/complete-topik-2-vocabulary-list-3900.html
- Íµ≠Î¶ΩÍµ≠Ïñ¥Ïõê: https://www.korean.go.kr/
- Korean morphological analyzers: KoNLPy, Mecab-ko
